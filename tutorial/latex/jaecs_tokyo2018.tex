%% \documentclass[handout,t]{beamer} % HANDOUT
%% \documentclass[handout,notes=show,t]{beamer} % NOTES
\documentclass[t]{beamer} % SLIDES

\usetheme{ZipfR}
\usepackage{beamer-tools}

\input{lib/math}  % basic mathematical notation
\input{lib/stat}  % notation for probability theory and statistics
\input{lib/vector}% convenience macros for vectors and matrices

\input{local/config} % local adjustments to configuration and macros

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% Titlepage

\title[JAECS 2018]{Measures of Productivity and Lexical Diversity}
\subtitle{\primary{7 October 2018}}
\author[Stefan Evert]{Stefan Evert\\ FAU Erlangen-NÃ¼rnberg}
\date[7 Oct 2018 | CC-by-sa]{\href{http://zipfr.r-forge.r-project.org/}{http://zipfr.r-forge.r-project.org/}\\
 \light{\small Licensed under CC-by-sa version 3.0}}

\begin{document}

\pgfdeclareimage[width=36mm]{jaecs-logo}{img/logo_jaecs}
\pgfdeclareimage[width=30mm]{kallimachos-logo}{img/logo_kallimachos}
\logo{\pgfuseimage{jaecs-logo}\hspace{6cm}\pgfuseimage{kallimachos-logo}}

\frame{\titlepage}
\hideLogo{}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section*{Outline}
\frame{ 
  \frametitle{Outline}
  \tableofcontents
}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Introduction}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Motivation}

\begin{frame}
  \frametitle{Research questions in computational corpus linguistics}

  \begin{itemize}
  \item How many words did Shakespeare know?
  \item What is the coverage of my treebank grammar on big data?
  \item How many typos are there on the Internet?
  \item Is \emph{-ness} more productive than \emph{-ity} in English?
  \item Are there differences in the productivity of nominal compounds between academic writing and novels?
  \item Does Dickens use a more complex vocabulary than Rowling?
  \item Can a decline in lexical complexity predict Alzheimer's disease?
  \item How frequent is a hapax legomenon from the Brown corpus?
  \item What is appropriate smoothing for my n-gram model?
  \item Who wrote the Bixby letter, Lincoln or Hay?
  \item How many different species of \ldots\ are there? \citep{Brainerd:82}
  \end{itemize}
\end{frame}

\begin{frame}
  \frametitle{Research questions in computational corpus linguistics}

  \begin{itemize}
  \item 
  \item \counterpoint{coverage estimates}
  \item
  \item
  \item \counterpoint{productivity}\\\rule{0mm}{1ex}
  \item \counterpoint{lexical complexity \& stylometry}
  \item 
  \item \counterpoint{prior \& posterior distribution}
  \item 
  \item \counterpoint{unexpected applications}
  \item 
  \end{itemize}
\end{frame}

\begin{frame}
  \frametitle{Type-token statistics}

  \begin{itemize}
  \item These applications relate \primary{token} and \primary{type} counts
    \begin{itemize}
    \item \hh{tokens} = individual instances (occurrences)
    \item \hh{types} = distinct items
    \end{itemize}
  \item Type-token statistics different from most statistical inference
    \begin{itemize}
    \item not about probability of a specific event
    \item but about diversity of events and their probability distribution
    \item[]\pause
    \end{itemize}
  \item Relatively little work in statistical science
  \item Nor a major research topic in computational linguistics
    \begin{itemize}
    \item very specialized, usually plays ancillary role in NLP
    \end{itemize}
  \item Corpus linguistics: TTR \& simple productivity measures
    \begin{itemize}
    \item often applied without any statistical inference
    \end{itemize}
  \end{itemize}
\end{frame}

\begin{frame}
  \frametitle{Zipf's law \citep{Zipf:49}}

  \begin{itemize}
  \item[A)] Frequency distributions in natural language are highly skewed
  \item[B)] Curious relationship between rank \& frequency
    \begin{center}\footnotesize
      \begin{tabular}[c]{lccc}
        word & $r$ & $f$ & $r\cdot f$ \\
        \midrule
        \emph{the} & 1. & 142,776 & 142,776 \\
        \emph{and} & 2. & 100,637 & 201,274 \\
	\emph{be}  & 3. &  94,181 & 282,543 \\
	\emph{of}  & 4. &  74,054 & 296,216
      \end{tabular}
      \counterpoint{(Dickens)}
    \end{center}
  \item[C)] Various explanations of Zipf's law
    \begin{itemize}
    \item principle of least effort \citep{Zipf:49}
    \item optimal coding system, MDL \citep{Mandelbrot:53,Mandelbrot:62}
    \item random sequences \citep{Miller:57,Li:92,Cao:etc:17}
    \item Markov processes $\so$ n-gram models \citep{Rouault:78}
    \end{itemize}
  \item[D)] Language evolution: birth-death-process \citep{Simon:55}
  \item[\hand] not the main topic today!
  \end{itemize}
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Notation \& basic concepts}

\newcommand{\TC}[1]{\counterpoint{\emph{#1}}}
\newcommand{\TL}[1]{\light{\emph{#1}}}
\newcommand<>{\TA}[1]{\light{\counterpoint#2{\emph{#1}}}}

\begin{frame}
  \frametitle{Tokens \& types}

  our sample: \TC{recently}, \TC{very}, \TC{not}, \TC{otherwise}, \TC{much}, \TC{very},
  \TC{very}, \TC{merely}, \TC{not}, \TC{now}, \TC{very}, \TC{much},
  \TC{merely}, \TC{not}, \TC{very}

  \begin{itemize}
  \item $N = 15$: number of \hh{tokens} = sample size
  \item $V = 7$: number of distinct \hh{types} = \h{vocabulary size}\\
    (\TC{recently}, \TC{very}, \TC{not}, \TC{otherwise}, \TC{much}, \TC{merely}, \TC{now})
  \end{itemize}

  \onslide<2->
  \begin{columns}[c]
    \begin{column}{5cm}
      \centering
      \h{type-frequency list}

      \begin{tabular}{l|c}
        $w$ & $f_w$ \\
        \hline
        \TC{recently} & 1 \\ 
        \TC{very}     & 5 \\
        \TC{not}      & 3 \\ 
        \TC{otherwise}& 1 \\ 
        \TC{much}     & 2 \\ 
        \TC{merely}   & 2 \\ 
        \TC{now}      & 1 
      \end{tabular}
    \end{column}
    \begin{column}{5cm}
      
    \end{column}
  \end{columns}
\end{frame}

\begin{frame}
  \frametitle{Zipf ranking}

  our sample: \TC{recently}, \TC{very}, \TC{not}, \TC{otherwise}, \TC{much}, \TC{very},
  \TC{very}, \TC{merely}, \TC{not}, \TC{now}, \TC{very}, \TC{much},
  \TC{merely}, \TC{not}, \TC{very}

  \begin{itemize}
  \item $N = 15$: number of \hh{tokens} = sample size
  \item $V = 7$: number of distinct \hh{types} = \h{vocabulary size}\\
    (\TC{recently}, \TC{very}, \TC{not}, \TC{otherwise}, \TC{much}, \TC{merely}, \TC{now})
  \end{itemize}

  \begin{columns}[c]
    \begin{column}{5cm}
      \centering
      \h{Zipf ranking}

      \begin{tabular}{l|c|c}
        $w$ & $r$ & $f_r$ \\
        \hline
        \TL{very}     & 1 & 5 \\
        \TL{not}      & 2 & 3 \\ 
        \TL{merely}   & 3 & 2 \\ 
        \TL{much}     & 4 & 2 \\ 
        \TL{now}      & 5 & 1 \\
        \TL{otherwise}& 6 & 1 \\ 
        \TL{recently} & 7 & 1 
      \end{tabular}
    \end{column}
    \begin{column}{5cm}
      \visible<2->{\includegraphics[width=45mm]{../plots/tutorial_tfl}}
    \end{column}
  \end{columns}
\end{frame}

\begin{frame}
  \frametitle{A realistic Zipf ranking: the Brown corpus}

  \gap
  \begin{scriptsize}
    \begin{tabular}{|r|r|l||r|r|l|}
      \hline
      \multicolumn{3}{|c||}{\hh{top frequencies}} & \multicolumn{3}{c|}{\hh{bottom frequencies}}\\
      \hline
      \textbf{\textit{r}} & \multicolumn{1}{c|}{\textbf{\textit{f}}} & \textbf{word} & \textbf{rank range} & \multicolumn{1}{c|}{\textbf{\textit{f}}} & \textbf{randomly selected examples}\\
      \hline
       1 & 69836 & the   &   7731 -- \phantom{0}8271 & 10 &    schedules, polynomials, bleak \\ 
       2 & 36365 & of    &   8272 -- \phantom{0}8922 &  9 &          tolerance, shaved, hymn \\ 
       3 & 28826 & and   &   8923 -- \phantom{0}9703 &  8 & decreased, abolish, irresistible \\ 
       4 & 26126 & to    &   9704 -- 10783 &  7 &        immunity, cruising, titan \\ 
       5 & 23157 & a     &  10784 -- 11985 &  6 &     geographic, lauro, portrayed \\ 
       6 & 21314 & in    &  11986 -- 13690 &  5 &     grigori, slashing, developer \\ 
       7 & 10777 & that  &  13691 -- 15991 &  4 &       sheath, gaulle, ellipsoids \\ 
       8 & 10182 & is    &  15992 -- 19627 &  3 &         mc, initials, abstracted \\ 
       9 &  9968 & was   &  19628 -- 26085 &  2 &         thar, slackening, deluxe \\ 
      10 &  9801 & he    &  26086 -- 45215 &  1 &  beck, encompasses, second-place \\ 
      \hline
    \end{tabular}
  \end{scriptsize}
\end{frame}

\begin{frame}
  \frametitle{A realistic Zipf ranking: the Brown corpus}

  \ungap[1]
  \begin{center}
    \only<beamer:1| handout:0>{\includegraphics[height=7.5cm]{../plots/tutorial_brown_tfl}}%
    \only<beamer:2| handout:1>{\includegraphics[height=7.5cm]{../plots/tutorial_brown_tfl_log}}%
  \end{center}
\end{frame}

\begin{frame}
  \frametitle{Frequency spectrum}

  \ungap[1.5]
  \begin{itemize}
  \item pool types with $f = 1$ (\primary{hapax legomena}), types with $f = 2$ (\primary{dis legomena}), \ldots, $f = m$, \ldots
  \item $V_1 = 3$: number of hapax legomena (\emph{now, otherwise, recently})
  \item $V_2 = 2$: number of dis legomena (\emph{merely, much})
  \item general definition: $V_m = \abs{\setdef{w}{f_w = m}}$
  \end{itemize}
  
  \begin{columns}[c]
    \begin{column}{35mm}
      \centering
      \textbf{Zipf ranking}

      \begin{tabular}{l|c|c}
        $w$ & $r$ & $f_r$ \\
        \hline
        \TL{very}     & 1 & 5 \\
        \TL{not}      & 2 & 3 \\ 
        \TL{merely}   & 3 & 2 \\ 
        \TL{much}     & 4 & 2 \\ 
        \TL{now}      & 5 & 1 \\
        \TL{otherwise}& 6 & 1 \\ 
        \TL{recently} & 7 & 1 
      \end{tabular}
    \end{column}
    \begin{column}{25mm}
      \centering
      \h{frequency\\ spectrum}

      \begin{tabular}{c|c}
        $m$ & $V_m$ \\
        \hline
        1 & 3 \\
        2 & 2 \\
        3 & 1 \\
        5 & 1
      \end{tabular}
    \end{column}
    \begin{column}{5cm}
      \visible<2->{\includegraphics[width=45mm]{../plots/tutorial_spc}}
    \end{column}
  \end{columns}

\end{frame}

\begin{frame}
  \frametitle{A realistic frequency spectrum: the Brown corpus}

  \ungap[1]
  \begin{center}
    \includegraphics[height=7.5cm]{../plots/tutorial_brown_spc}%
  \end{center}
\end{frame}

\begin{frame}
  \frametitle{Vocabulary growth curve}
  
    our sample: \TA<1->{recently}, \TA<2->{very}, \TA<2->{not}, \TA<3->{otherwise}, \TA<3->{much}, \TA<3->{very},
  \TA<3->{very}, \TA<4->{merely}, \TA<4->{not}, \TA<4->{now}, \TA<4->{very}, \TA<4->{much},
  \TA<5->{merely}, \TA<5->{not}, \TA<5->{very}

  \begin{columns}[c]
    \begin{column}{6cm}
      \begin{itemize}
      \item<1-> $N = 1$, $V(N) = 1$, $V_1(N) = 1$
      \item<2-> $N = 3$, $V(N) = 3$, $V_1(N) = 3$
      \item<3-> $N = 7$, $V(N) = 5$, $V_1(N) = 4$
      \item<4-> $N = 12$, $V(N) = 7$, $V_1(N) = 4$
      \item<5-> $N = 15$, $V(N) = 7$, $V_1(N) = 3$
      \end{itemize}
    \end{column}
    \begin{column}{5cm}
      \visible<6->{\includegraphics[width=5cm]{../plots/tutorial_vgc}}%
    \end{column}
  \end{columns}

\end{frame}

\begin{frame}
  \frametitle{A realistic vocabulary growth curve: the Brown corpus}

  \ungap[1]
  \begin{center}
    \includegraphics[height=6.5cm]{../plots/tutorial_brown_vgc}%
  \end{center}
\end{frame}

\begin{frame}
  \frametitle{Vocabulary growth in authorship attribution}

  \ungap[1]
  \begin{itemize}
  \item Authorship attribution by n-gram tracing applied to the case of the Bixby letter \citep{Grieve:etc:18}
  \item Word or character n-grams in disputed text are compared against large ``training'' corpora from candidate authors
  \end{itemize}

  \begin{center}
    \includegraphics[width=8cm]{img/GrieveEtc2018_fig1}
  \end{center}
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Zipf's law}

\begin{frame}
  \frametitle{Observing Zipf's law}
  \framesubtitle{across languages and different linguistic units}

  \ungap[1.5]
  \begin{center}
    \includegraphics[height=7.5cm]{img/othercorporarf}
  \end{center}
\end{frame}

\begin{frame}
  \frametitle{Observing Zipf's law}

  \ungap[1.5]
  \begin{center}
    \only<beamer:1| handout:0>{\includegraphics[height=7.5cm]{../plots/tutorial_brown_tfl_log}}%
    \only<beamer:2| handout:1>{\includegraphics[height=7.5cm]{../plots/tutorial_brown_tfl_loglog}}%
  \end{center}
\end{frame}


\begin{frame}
  \frametitle{Observing Zipf's law}

  \begin{itemize}
  \item Straight line in double-logarithmic space corresponds\\
    to \h{power law} for original variables
  \item This leads to Zipf's (\citeyear{Zipf:49,Zipf:65}) famous law:
    \[
      f_r = \frac{C}{r^a}
    \]
  \item<2-> If we take logarithm on both sides, we obtain:
    \[
    \only<beamer:2| handout:0>{%
      \log f_r = \log C - a \cdot \log r}%
    \only<beamer:3-| handout:1>{%
      \secondary{\underbrace{\log f_r}_y}
      = \log C - a \cdot \secondary{\underbrace{\log r}_x}}%
    \]
  \item<4-> Intuitive interpretation of $a$ and $C$:
    \begin{itemize}
    \item $a$ is \h{slope} determining how fast log frequency decreases
    \item $\log C$ is \h{intercept}, i.e.\ log frequency of most frequent word
      ($r = 1$ \so $\log r = 0$) 
    \end{itemize}
  \end{itemize}

\end{frame}

\begin{frame}
  \frametitle{Observing Zipf's law}
  \framesubtitle{Least-squares fit = linear regression in log-space (Brown corpus)}

  \ungap[1.5]
  \begin{center}
    \includegraphics[height=7.5cm]{img/brown-zipf-rf}
  \end{center}
\end{frame}

\begin{frame}
  \frametitle{Zipf-Mandelbrot law}
  \framesubtitle{\citet{Mandelbrot:53,Mandelbrot:62}}

  \begin{itemize}
    \item Mandelbrot's extra parameter:
    \[
    f_r = \frac{C}{(r + b)^a}
    \]
  \item Zipf's law is special case with $b=0$
  \item<2-> Assuming $a=1$, $C=$ 60,000, $b=1$:
    \begin{itemize}
    \item For word with rank 1, Zipf's law predicts frequency of
      60,000; Mandelbrot's variation predicts frequency of 30,000
    \item For word with rank 1,000,  Zipf's law predicts frequency of
      60; Mandelbrot's variation predicts frequency of 59.94
    \item[]
    \end{itemize}
  \item<3-> Zipf-Mandelbrot law forms basis of statistical LNRE models% 
    \begin{itemize}
    \item ZM law derived mathematically as limiting distribution of
      vocabulary generated by a character-level Markov process
    \end{itemize}
  \end{itemize}
\end{frame}

\begin{frame}
  \frametitle{Zipf-Mandelbrot law} 
  \framesubtitle{Non-linear least-squares fit (Brown corpus)}

  \ungap[1.5]
  \begin{center}
    \includegraphics[height=7.5cm]{img/brown-zipf-man-rf}
  \end{center}
\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Measuring productivity}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Productivity \& lexical diversity}

\begin{frame}
  \frametitle{Measuring morphological productivity}
  \framesubtitle{example from \citet{Evert:Luedeling:01}}
  
  \centering
  \includegraphics[width=11cm]{../plots/EL2001_vgc_bar_sam_oes}%
\end{frame}

\begin{frame}
  \frametitle{Quantitative measures of productivity}
  \framesubtitle{\citep{Tweedie:Baayen:98,Baayen:01}}

  \footnotesize
  \begin{columns}[c]
        \begin{column}{6cm}
      \ungap[1.2]
      \begin{itemize}
      \item Baayen's (\citeyear{Baayen:91}) productivity index $\mathcal{P}$
        \[
        \mathcal{P} = \frac{V_1}{N}
        \]
      \item TTR = type-token ratio
        \[
        \text{TTR} = \frac{V}{N}
        \]
      \item Slope $a$ of Zipf-Mandelbrot law
      \item Population size
        \[
        S = \lim_{N \to \infty} V(N)
        \]
      \item Herdan's law (\citeyear{Herdan:64})
        \[
        C = \frac{\log V}{\log N}
        \]
      \end{itemize}
    \end{column}
    \begin{column}{6cm}
      \begin{itemize}
      \item<2-> \citet{Yule:44} /  Simpson (1949) 
        \[
          K = 10\,000\cdot \frac{\sum_m m^2 V_m - N}{N^2}
        \]
      \item<2-> Guiraud (1954)
        \[
          R = \frac{V}{\sqrt{N}}
        \]
      \item<2-> \citet{Sichel:75}
        \[
          S = \frac{V_2}{V}
        \]
      \item<2-> HonorÃ© (1979)
        \[
          H = \frac{\log N}{1 - \frac{V_1}{V}}
        \]
      \end{itemize}
    \end{column}
  \end{columns}  
\end{frame}

\begin{frame}[c]
  \frametitle{Productivity measures for bare singulars in the BNC}
  %% \framesubtitle{}

  \begin{columns}[c]
    \begin{column}{6cm}
      \centering
      \begin{tabular}{c@{$\qquad$}r@{$\qquad$}r}
        \toprule
        &    spoken &   written \\
        \midrule
        $V$     &  2,039 & 12,876 \\
        $N$     &  6,766 & 85,750 \\
        \midrule
        $K$     &    86.84 &    28.57 \\
        $R$     &    24.79 &    43.97 \\
        $S$     &     0.13 &     0.15 \\
        $C$     &     0.86 &     0.83 \\
        $\mathcal{P}$     &     0.21 &     0.08 \\
        TTR   &     0.301 &     0.150 \\
        $a$     &     1.18 &     1.27 \\
        pop.\ $S$ & 15,958 & 36,874 \\
        \bottomrule
      \end{tabular}
    \end{column}
    \begin{column}{6cm}
      \visible<2->{\includegraphics[width=6cm]{img/bare_bncWS_vgc}}
    \end{column}
  \end{columns}
\end{frame}

\begin{frame}[c]
  \frametitle{Are these ``lexical constants'' really constant?}
  %% \framesubtitle{}

  \centering
  \includegraphics[width=11cm]{img/bare_bncS_obs_lexical_constants}
\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{LNRE models without the math}

\begin{frame}
  \frametitle{Motivation}

  \begin{itemize}
  \item Often need to compare samples of different sizes
    \begin{itemize}
    \item[\hand] extrapolation of VGC \& productivity measures
    \item[]
    \end{itemize}
  \item<2-> Interested in productivity of affix, vocabulary of author, \ldots; not in a particular text or sample\\
    \begin{itemize}
    \item[\hand] statistical inference from sample to population
    \item[\hand] significance of differences in productivity
    \item[]
    \end{itemize}
  \item<3-> Discrete frequency counts are difficult to capture with generalizations such as Zipf's law
    \begin{itemize}
    \item[\hand] Zipf's law predicts many impossible types with $1 < f_r < 2$
    \item[\hand] population does not suffer from such quantization effects
    \item[]
    \end{itemize}
  \item[\So] Specialized LNRE models \citep{Baayen:01}
    \begin{itemize}
    \item LNRE = Large Number of Rare Events
    \end{itemize}
  \end{itemize}
\end{frame}

\begin{frame}
  \frametitle{The LNRE population}

  \begin{itemize}
  \item Population: set of $S$ types $w_i$ with occurrence \h{probabilities} $\pi_i$
  \item $S$ = \h{population diversity} can be finite or infinite ($S = \infty$)
  \item Not interested in specific types \so  arrange by decreasing
    probability: $\pi_1\geq \pi_2\geq \pi_3 \geq \cdots$
    \begin{itemize}
    \item[\hand] impossible to determine probabilities of all individual types
    \end{itemize}
  \item Normalization: $\pi_1 + \pi_2 + \ldots + \pi_S = 1$
    \begin{itemize}
    \item[]
    \end{itemize}
  \item Need \hh{parametric} statistical \hh{model} to describe full population (esp.\ for $S = \infty$),
    i.e.\ a function $i \mapsto \pi_i$
    \begin{itemize}
    \item type probabilities $\pi_i$ cannot be estimated reliably from a sample, but parameters of this function can
    \item NB: population index $i$ $\neq$ Zipf rank $r$
    \end{itemize}
  \end{itemize}
\end{frame}

\begin{frame}
  \frametitle{Zipf-Mandelbrot law as a population model}

  \begin{itemize}
  \item Zipf-Mandelbrot law for type probabilities:
    \[ \pi_i := \frac{C}{(i + b) ^ a} \]
  \item Two free parameters: $a > 1$ and $b \geq 0$
    \begin{itemize}
    \item[\hand] $C$ is not a parameter but a normalization constant,\\
      needed to ensure that $\sum_i \pi_i = 1$
    \end{itemize}
  \item Third parameter: $S > 0$ or $S = \infty$
  \item[]
  \item This is the \h{Zipf-Mandelbrot} population model \citep{Evert:04}
    \begin{itemize}
    \item \hh{ZM} for Zipf-Mandelbrot model ($S = \infty$)
    \item \hh{fZM} for finite Zipf-Mandelbrot model
    \end{itemize}
  \end{itemize}
\end{frame}

\begin{frame}
  \frametitle{The parameters of the Zipf-Mandelbrot model}

  \ungap[1.5]
  \begin{center}
    \begin{tabular}{cc}
      \includegraphics[width=40mm]{img/05-models-zm-log-4} &
      \includegraphics[width=40mm]{img/05-models-zm-log-1} \\
      \includegraphics[width=40mm]{img/05-models-zm-log-3} &
      \includegraphics[width=40mm]{img/05-models-zm-log-2}
    \end{tabular}
  \end{center}
\end{frame}

\begin{frame}
  \frametitle{Sampling from a population model}

  \begin{center}
    \footnotesize
    \begin{tabular}{r *{9}{ @{\hspace{1mm}}r } r}
      \hh{\#1:} &   1 &  42 &  34 &  23 & 108 &  18 &  48 &  18 &   1 & \ldots \\ 
       \pause
      & time & order & room & school & town & course & area & course & time & \ldots \\
       \pause\\
      \hh{\#2:} & 286 &  28 &  23 &  36 &   3 &   4 &   7 &   4 &   8 & \ldots \\ 
       \pause\\
      \hh{\#3:} &   2 &  11 & 105 &  21 &  11 &  17 &  17 &   1 &  16 & \ldots \\ 
       \pause\\
      \hh{\#4:} &  44 &   3 & 110 &  34 & 223 &   2 &  25 &  20 &  28 & \ldots \\ 
       \\
      \hh{\#5:} &  24 &  81 &  54 &  11 &   8 &  61 &   1 &  31 &  35 & \ldots \\ 
      \\
      \hh{\#6:} &   3 &  65 &   9 & 165 &   5 &  42 &  16 &  20 &   7 & \ldots \\ 
      \\
      \hh{\#7:} &  10 &  21 &  11 &  60 & 164 &  54 &  18 &  16 & 203 & \ldots \\ 
      \\
      \hh{\#8:} &  11 &   7 & 147 &   5 &  24 &  19 &  15 &  85 &  37 & \ldots \\
      \\
      \vdots & \vdots & \vdots & \vdots & \vdots & \vdots & \vdots & \vdots & \vdots & \vdots
    \end{tabular}
  \end{center}
\end{frame}

\begin{frame}
  \frametitle{Samples: type frequency list \& spectrum}

  \ungap[1]
  \begin{center}
    \begin{tabular}[t]{r | rr}
      rank $r$ & $f_r$ & type $i$ \\
      \hline
       1 & 37 &  6 \\
       2 & 36 &  1 \\
       3 & 33 &  3 \\
       4 & 31 &  7 \\
       5 & 31 & 10 \\
       6 & 30 &  5 \\
       7 & 28 & 12 \\
       8 & 27 &  2 \\
       9 & 24 &  4 \\
      10 & 24 & 16 \\
      11 & 23 &  8 \\
      12 & 22 & 14 \\
      \vdots & \vdots & \vdots
    \end{tabular}
    \hspace{2cm}
    \begin{tabular}[t]{r | r}
      $m$ & $V_m$ \\
      \hline
       1 & 83 \\
       2 & 22 \\
       3 & 20 \\
       4 & 12 \\
       5 & 10 \\
       6 &  5 \\
       7 &  5 \\
       8 &  3 \\
       9 &  3 \\
      10 &  3 \\
      \vdots & \vdots \\
      \multicolumn{2}{c}{} \\
      \multicolumn{2}{c}{\hh{sample \#1}}
    \end{tabular}
  \end{center}
\end{frame}

\begin{frame}
  \frametitle{Samples: type frequency list \& spectrum}

  \ungap[1]
  \begin{center}
    \begin{tabular}[t]{r | rr}
      rank $r$ & $f_r$ & type $i$ \\
      \hline
       1 & 39 &  2 \\
       2 & 34 &  3 \\
       3 & 30 &  5 \\
       4 & 29 & 10 \\
       5 & 28 &  8 \\
       6 & 26 &  1 \\
       7 & 25 & 13 \\
       8 & 24 &  7 \\
       9 & 23 &  6 \\
      10 & 23 & 11 \\
      11 & 20 &  4 \\
      12 & 19 & 17 \\
      \vdots & \vdots & \vdots
    \end{tabular}
    \hspace{2cm}
    \begin{tabular}[t]{r | r}
      $m$ & $V_m$ \\
      \hline
       1 & 76 \\
       2 & 27 \\
       3 & 17 \\
       4 & 10 \\
       5 &  6 \\
       6 &  5 \\
       7 &  7 \\
       8 &  3 \\
      10 &  4 \\
      11 &  2 \\
      \vdots & \vdots \\
      \multicolumn{2}{c}{} \\
      \multicolumn{2}{c}{\hh{sample \#2}}
    \end{tabular}
  \end{center}
\end{frame}

\begin{frame}
  \frametitle{Random variation in type-frequency lists}

  \ungap[1]
  \begin{tabular}{ccc}
    \includegraphics[width=40mm]{img/05-samples-tfl-r-1} &
    \includegraphics[width=40mm]{img/05-samples-tfl-r-2} &
    \raisebox{2cm}{$r\leftrightarrow f_r$} \\
    
    \includegraphics[width=40mm]{img/05-samples-tfl-k-1} &
    \includegraphics[width=40mm]{img/05-samples-tfl-k-2} &
    \raisebox{2cm}{$i\leftrightarrow f_i$} 
  \end{tabular}
\end{frame}

\begin{frame}
  \frametitle{Random variation: frequency spectrum}

  \ungap[1]
  \begin{center}
    \only<beamer:0| handout:1>{%
      \begin{tabular}{cc}
        \includegraphics[width=40mm]{img/05-samples-spc-1} &
        \includegraphics[width=40mm]{img/05-samples-spc-2} \\
        \includegraphics[width=40mm]{img/05-samples-spc-3} &
        \includegraphics[width=40mm]{img/05-samples-spc-4} 
      \end{tabular}%
    }%
    \only<beamer:1| handout:0>{\includegraphics[width=70mm]{img/05-samples-spc-1}}%
    \only<beamer:2| handout:0>{\includegraphics[width=70mm]{img/05-samples-spc-2}}%
    \only<beamer:3| handout:0>{\includegraphics[width=70mm]{img/05-samples-spc-3}}%
    \only<beamer:4| handout:0>{\includegraphics[width=70mm]{img/05-samples-spc-4}}%
  \end{center}
\end{frame}

\begin{frame}
  \frametitle{Random variation: vocabulary growth curve}

  \ungap[1]
  \begin{center}
    \only<beamer:0| handout:1>{%
      \begin{tabular}{cc}
        \includegraphics[width=40mm]{img/05-samples-vgc-1} &
        \includegraphics[width=40mm]{img/05-samples-vgc-2} \\
        \includegraphics[width=40mm]{img/05-samples-vgc-3} &
        \includegraphics[width=40mm]{img/05-samples-vgc-4} 
      \end{tabular}%
    }%
    \only<beamer:1| handout:0>{\includegraphics[width=70mm]{img/05-samples-vgc-1}}%
    \only<beamer:2| handout:0>{\includegraphics[width=70mm]{img/05-samples-vgc-2}}%
    \only<beamer:3| handout:0>{\includegraphics[width=70mm]{img/05-samples-vgc-3}}%
    \only<beamer:4| handout:0>{\includegraphics[width=70mm]{img/05-samples-vgc-4}}%
  \end{center}
\end{frame}


\begin{frame}
  \frametitle{Expected values}

  \begin{itemize}
  \item There is no reason why we should choose a particular sample to compare
    to the real data or make a prediction -- each one is equally likely or
    unlikely
  \item Take the average over a large number of samples, called \h{expected
      value} or \h{expectation} in statistics
  \item[]
  \item Notation: $\text{E}\bigl[V(N)\bigr]$ and $\text{E}\bigl[V_m(N)\bigr]$
    \begin{itemize}
    \item indicates that we are referring to expected values for a sample of
      size $N$
    \item rather than to the specific values $V$ and $V_m$\\
      observed in a particular sample or a real-world data set
    \item[]
    \end{itemize}
  \item Expected values can be calculated efficiently \emph{without}
    generating thousands of random samples
  \end{itemize}
\end{frame}

\begin{frame}
  \frametitle{The expected frequency spectrum}

  \ungap[1]
  \begin{center}
    \only<beamer:0| handout:1>{%
      \begin{tabular}{cc}
        \includegraphics[width=40mm]{img/05-samples-spc-exp-vs-sample-1} &
        \includegraphics[width=40mm]{img/05-samples-spc-exp-vs-sample-2} \\
        \includegraphics[width=40mm]{img/05-samples-spc-exp-vs-sample-3} &
        \includegraphics[width=40mm]{img/05-samples-spc-exp-vs-sample-4} 
      \end{tabular}%
    }%
    \only<beamer:1| handout:0>{\includegraphics[width=70mm]{img/05-samples-spc-exp-vs-sample-1}}%
    \only<beamer:2| handout:0>{\includegraphics[width=70mm]{img/05-samples-spc-exp-vs-sample-2}}%
    \only<beamer:3| handout:0>{\includegraphics[width=70mm]{img/05-samples-spc-exp-vs-sample-3}}%
    \only<beamer:4| handout:0>{\includegraphics[width=70mm]{img/05-samples-spc-exp-vs-sample-4}}%
  \end{center}
\end{frame}

\begin{frame}
  \frametitle{The expected vocabulary growth curve}

  \ungap[1]
  \begin{center}
    \begin{tabular}{c @{} c}
      \includegraphics[width=50mm]{img/05-samples-vgc-exp-vs-samples} &
      \includegraphics[width=50mm]{img/05-samples-vgc-V1-exp-vs-samples}
    \end{tabular}
  \end{center}
\end{frame}

\begin{frame}
  \frametitle{Prediction intervals for the expected VGC}

  \ungap[1]
  \begin{center}
    \begin{tabular}{c @{} c}
      \includegraphics[width=50mm]{img/05-samples-vgc-exp-vs-samples-conf} &
      \includegraphics[width=50mm]{img/05-samples-vgc-V1-exp-vs-samples-conf}
    \end{tabular}
  \end{center}

  ``Confidence intervals'' indicate predicted sampling distribution:%
  \begin{itemize}
  \item[\hand] for 95\% of samples generated by the LNRE model, VGC will fall within the range delimited by the thin red lines
  \end{itemize}

\end{frame}

\begin{frame}<beamer:1-7| handout:1-3,7>
  \frametitle{Parameter estimation by trial \& error}

  \begin{center}
    \begin{tabular}{c @{} c}
      \only<beamer:1| handout:1>{\includegraphics[width=50mm]{img/05-estimation-spc-1}}%
      \only<beamer:2| handout:2>{\includegraphics[width=50mm]{img/05-estimation-spc-2}}%
      \only<beamer:3| handout:3>{\includegraphics[width=50mm]{img/05-estimation-spc-3}}%
      \only<beamer:4| handout:0>{\includegraphics[width=50mm]{img/05-estimation-spc-1}}%
      \only<beamer:5| handout:0>{\includegraphics[width=50mm]{img/05-estimation-spc-4}}%
      \only<beamer:6| handout:0>{\includegraphics[width=50mm]{img/05-estimation-spc-5}}%
      \only<beamer:7| handout:7>{\includegraphics[width=50mm]{img/05-estimation-spc-6}}%
      &
      \only<beamer:1| handout:1>{\includegraphics[width=50mm]{img/05-estimation-vgc-1}}%
      \only<beamer:2| handout:2>{\includegraphics[width=50mm]{img/05-estimation-vgc-2}}%
      \only<beamer:3| handout:3>{\includegraphics[width=50mm]{img/05-estimation-vgc-3}}%
      \only<beamer:4| handout:0>{\includegraphics[width=50mm]{img/05-estimation-vgc-1}}%
      \only<beamer:5| handout:0>{\includegraphics[width=50mm]{img/05-estimation-vgc-4}}%
      \only<beamer:6| handout:0>{\includegraphics[width=50mm]{img/05-estimation-vgc-5}}%
      \only<beamer:7| handout:7>{\includegraphics[width=50mm]{img/05-estimation-vgc-6}}%
    \end{tabular}
  \end{center}
\end{frame}

\begin{frame}
  \frametitle{Automatic parameter estimation}
  % \framesubtitle{Minimisation of suitable cost function for frequency spectrum}

  \begin{center}
    \begin{tabular}{c @{} c}
      \includegraphics[width=50mm]{img/05-estimation-spc-estimated} &
      \includegraphics[width=50mm]{img/05-estimation-vgc-estimated} 
    \end{tabular}
  \end{center}

  \ungap[1]
  \begin{itemize}
    \item By trial \& error we found $a=2.0$ and $b=550$
    \item Automatic estimation procedure: $a=2.39$ and $b=1968$
  \end{itemize}
\end{frame}

\begin{frame}[c]
  \frametitle{Automatic parameter estimation}

  \centering
  \hspace*{-5mm}%
  \only<beamer:1| handout:1>{\includegraphics[width=12cm]{img/cost_bncBare_chisq_10}}%
  \only<beamer:2| handout:0>{\includegraphics[width=12cm]{img/cost_brownWords_chisq_5}}%
\end{frame}

\begin{frame}
  \frametitle{Extrapolation of vocabulary growth curves}
  
  \centering\ungap[1]
  \includegraphics[width=8cm]{img/02-samples-ot-vgc-extrapolated}
\end{frame}

\begin{frame}
  \frametitle{Measuring morphological productivity}
  \framesubtitle{example from \citet{Evert:Luedeling:01}}
  
  \centering
  \only<beamer:1| handout:0>{\includegraphics[width=11cm]{../plots/EL2001_vgc_bar_sam_oes}}%
  \only<beamer:2| handout:1>{\includegraphics[width=11cm]{../plots/EL2001_vgc_bar_sam_oes_extrap}}%
\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 
\section{Challenges}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 
\subsection{Extrapolation accuracy \& non-randomness}

\begin{frame}[c]
  \frametitle{How accurate is LNRE-based extrapolation?}
  \framesubtitle{\citep{Baroni:Evert:05}}

  \hspace*{-10mm}%
  \only<beamer:1| handout:1>{\includegraphics[width=12.5cm]{img/BE2005_bar-4-2}}%
  \only<beamer:2| handout:0>{\includegraphics[width=12.5cm]{img/BE2005_lich-4-2}}%
  \only<beamer:3| handout:2>{\includegraphics[width=12.5cm]{img/BE2005_lob-4-2}}%
  \only<beamer:4| handout:3>{\includegraphics[width=12.5cm]{img/BE2005_bnc-100-10}}%
\end{frame}

\begin{frame}
  \frametitle{Reasons for poor extrapolation quality}

  \begin{itemize}
  \item Major problem: \h{non-randomness} of corpus data
    \begin{itemize}
    \item LNRE modelling assumes that corpus is random sample
    \item[]
    \end{itemize}
  \item<2-> Cause 1: \hh{repetition} within texts
    \begin{itemize}
    \item most corpora use entire text as unit of sampling
    \item also referred to as ``term clustering'' or ``burstiness''
    \item well-known in computational linguistics \citep{Church:00}
    \item[]
    \end{itemize}
  \item<3-> Cause 2: \hh{non-homogeneous} corpus
    \begin{itemize}
    \item cannot extrapolate from spoken BNC to written BNC 
    \item similar for different genres and domains
    \item also within single text, e.g.\ beginning/end of novel
    \end{itemize}
  \end{itemize}
\end{frame}

\begin{frame}
  \frametitle{The ECHO correction}
  \framesubtitle{\citep{Baroni:Evert:07a}}

  \begin{itemize}
  \item Empirical study: quality of extrapolation $N_0 \to 4 N_0$ starting from random samples of corpus texts
  \end{itemize}

  \begin{center}
    \begin{tabular}{cc}
      \only<beamer:1| handout:1>{\includegraphics[height=4.5cm]{img/BE2007_plain_ranges_V_dewac}}%
      \only<beamer:2| handout:0>{\includegraphics[height=4.5cm]{img/BE2007_plain_ranges_V_bnc}}%
      & \includegraphics[width=5cm]{img/BE2007_plain_corr_V_3N}
    \end{tabular}
  \end{center}
\end{frame}

\begin{frame}
  \frametitle{The ECHO correction}
  \framesubtitle{\citep{Baroni:Evert:07a}}

  \begin{itemize}
  \item ECHO correction: replace every repetition within same text by special type \textsc{echo} (= document frequencies)
  \end{itemize}

  \ungap[1]
  \begin{center}
    \begin{tabular}{cc}
      \only<beamer:1| handout:1>{\includegraphics[height=4.5cm]{img/BE2007_ranges_V_dewac}}%
      \only<beamer:2| handout:0>{\includegraphics[height=4.5cm]{img/BE2007_ranges_V_bnc}}%
      & \includegraphics[width=5cm]{img/BE2007_corr_V_3N}
    \end{tabular}
  \end{center}  
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 
\subsection{Parameter estimation for small samples}

\begin{frame}
  \frametitle{Bootstrapping}

  \begin{itemize}
  \item<1-> An empirical approach to sampling variation:
    \begin{itemize}
    \item take many random samples from the same population
    \item train LNRE model on each sample
    \item analyse distribution of model parameters, goodness-of-fit, etc.
      (mean, median, s.d., boxplot, histogram, \ldots)
    \item problem: how to obtain the additional samples?
    \end{itemize}
  \item<2-> Bootstrapping \citep{Efron:79}
    \begin{itemize}
    \item resample from observed data \emph{with replacement}
    \item this approach is not suitable for type-token distributions
      (resamples underestimate vocabulary size $V$ \so biased)
    \end{itemize}
  \item<3-> Parametric bootstrapping
    \begin{itemize}
    \item use fitted model to generate samples, i.e.\ sample from the population described by the model
    \item advantage: ``correct'' parameter values are known
    \end{itemize}
  \end{itemize}
\end{frame}

\begin{frame}
  \frametitle{Bootstrapping}
  \framesubtitle{parametric bootstrapping with 100 replicates, fZM samples for $N = 3467$}

  \ungap[.5]
  \only<beamer:1| handout:1>{\h{Zipfian slope $a = 1 / \alpha$}}%
  \only<beamer:2| handout:0>{\h{Offset $b = (1 - \alpha) / (B\cdot \alpha)$}}%
  \only<beamer:3| handout:0>{\h{fZM probability cutoff $A = \pi_S$}}%
  \only<beamer:4| handout:2>{\h{Goodness-of-fit statistic $X^2$} \secondary{(model not plausible for $X^2 > 11$)}}%
  \only<beamer:5-6| handout:3>{\h{Population diversity $S$}}%
  \ungap[.5]
  \begin{center}
    \only<beamer:1| handout:1>{\includegraphics[width=10cm]{img/05-bootstrap-ultra-alpha}}%
    \only<beamer:2| handout:0>{\includegraphics[width=10cm]{img/05-bootstrap-ultra-B}}%
    \only<beamer:3| handout:0>{\includegraphics[width=10cm]{img/05-bootstrap-ultra-A}}%
    \only<beamer:4| handout:2>{\includegraphics[width=10cm]{img/05-bootstrap-ultra-X2}}%
    \only<beamer:5| handout:0>{\includegraphics[width=10cm]{img/05-bootstrap-ultra-S}}%
    \only<beamer:6| handout:3>{\includegraphics[width=10cm]{img/05-bootstrap-ultra-S-zoomed}}%
  \end{center}
\end{frame}

\begin{frame}[c]
  \frametitle{Sample size matters!}
  \framesubtitle{Brown corpus too small for reliable LNRE parameter estimation on bare singulars}

  \centering
  \includegraphics[width=11cm]{img/bare_brown_boot_fzm_a_S}
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 
\subsection{How meaningful are productivity measures?}

\begin{frame}[c]
  \frametitle{Empirical observations}
  %% \framesubtitle{}

  \centering
  \includegraphics[width=11cm]{img/bare_bncS_obs_lexical_constants}
\end{frame}

\begin{frame}
  \frametitle{Parametric bootstrapping with LNRE models}

  \begin{columns}[T]
    \begin{column}{55mm}
      \begin{itemize}
      \item Use simulation experiments to gain better understanding of quantitative measures
      \item Resampling (bootstrapping) leads to biased type counts
      \item<2->[\So] Parametric bootstrapping based on LNRE population
        \begin{itemize}
        \item arbitrary sample size
        \item intuitive notion of productivity \so parameters 
        \item controlled manipulation of confounding factors
        \end{itemize}
      \end{itemize}
    \end{column}
    \begin{column}{45mm}
      \visible<2->{\includegraphics[width=45mm]{img/lexconst_spectra}}%
    \end{column}
  \end{columns}
\end{frame}

\begin{frame}[c]
  \frametitle{Parametric bootstrapping: sample size}
  
  \centering
  \includegraphics[width=11cm]{img/lexconst_sample_size}
\end{frame}

\begin{frame}[c]
  \frametitle{Parametric bootstrapping: frequent lexicalized types}

  \centering
  \includegraphics[width=11cm]{img/lexconst_echo_type}
\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 
\subsection{A proposal}

\begin{frame}
  \frametitle{Case study: Iris Murdoch \& early symptoms of AD}
  \framesubtitle{\citep{Evert:Wankerl:Noeth:17}}

  \ungap[1]
  \begin{itemize}
  \item Renowned British author (1919--1999)
  \item Published a total of 26 novels, mostly well received by critics
  \item Murdoch experienced unexpected difficulties composing her last novel, received ``without enthusiasm'' \citep{Garrard:etc:05}
  \item Diagnosis of Alzheimer's disease shortly after publication
  \end{itemize}

  \gap[.5]
  \begin{columns}[c]
    \begin{column}{5cm}
      \onslide<2->
      \primary{Conflicting results:}
      \begin{itemize}
      \item Decline of lexical diversity in last novel\\
        \citep{Garrard:etc:05,Pakhomov:etc:11}
      \item No clear effects found\\ \citep{Le:etc:11}
      \end{itemize}
    \end{column}
    \begin{column}{4.8cm}
      \tiny\onslide<1->
      \includegraphics[width=4.8cm]{img/murdoch_bbc_story}\\
      \secondary{\hfill
        \href{http://news.bbc.co.uk/2/hi/health/4058605.stm}{http://news.bbc.co.uk/2/hi/health/4058605.stm}}
    \end{column}
  \end{columns}
\end{frame}

\begin{frame}
  \frametitle{Case study: Iris Murdoch \& early symptoms of AD}
  \framesubtitle{\citep{Evert:Wankerl:Noeth:17}}

  \begin{itemize}
  \item Corpus data
    \begin{itemize}
    \item 19 out of 26 novels written by Iris Murdoch
    \item including 9 last novels, spanning a period of almost 20 years
    \item acquired as e-books (no errors due to OCR)
    \item[]
    \end{itemize}
  \item Pre-processing and annotation
    \begin{itemize}
    \item Stanford CoreNLP \citep{Manning:etc:14} for tokenization, sentence
      splitting, POS tagging, and syntactic parsing
    \item exclude dialogue based on typographic quotation marks
      \citep[following][]{Garrard:etc:05,Pakhomov:etc:11}
    \item[]
    \end{itemize}
  \item The challenge
    \begin{itemize}
    \item[\hand] assess significance of differences in productivity \primary{for single texts}
    \item[\hand] might explain conflicting results in prior work
    \end{itemize}
  \end{itemize}
\end{frame}

\begin{frame}[c]
  \frametitle{Measures of vocabulary diversity = productivity}
  \framesubtitle{\citep{Evert:Wankerl:Noeth:17}}

  \centering
  \hspace*{-8mm}%
  \begin{tabular}{@{}cc@{}}
    \only<beamer:1-2| handout:0>{\includegraphics[width=6cm]{img/murdoch_kappa_raw}}%
    \only<beamer:3-| handout:1>{\includegraphics[width=6cm]{img/murdoch_V_raw}}%
    & \visible<2->{\includegraphics[width=6cm]{img/murdoch_H_raw}}%
    \\[5mm]
    \only<beamer:1-2| handout:0>{\secondary{Yule's $\kappa$}}%
    \only<beamer:3-| handout:1>{\secondary{type count / TTR}}%
    & \visible<2->{\secondary{HonorÃ© $H$}}%
  \end{tabular}                                     
\end{frame}

\begin{frame}
  \frametitle{Cross-validation for productivity measures}
  \framesubtitle{\citep{Evert:Wankerl:Noeth:17}}

  \secondary{As a first step:}
  \begin{itemize}
  \item Partition each novel into folds of 10,000 consecutive tokens
  \item[\So] $k \ge 6$ folds for each novel (leftover tokens discarded)
  \end{itemize}
  
  \onslide<2->
  \secondary{Then:}
  \begin{itemize}
  \item<2-> Evaluate complexity measure of interest on each fold
    \[ y_1, \dots, y_k \]
  \item<3-> Compute macro-average as overall measure for the entire text
    \[ \bar{y} = \frac{y_1+\dots + y_k}{k} \]
  \item<3-> Instead of value $x$ obtained by evaluating measure on full text
  \end{itemize}
\end{frame}

\begin{frame}
  \frametitle{Cross-validation for productivity measures}
  \framesubtitle{\citep{Evert:Wankerl:Noeth:17}}

  \secondary{Significance testing procedure:}
  \begin{itemize}
  \item Standard deviation $\sigma$ of individual folds estimated from data
    \[ \sigma^2 \approx s^2 = \frac{1}{k - 1} \sum_{i=1}^k (y_i - \bar{y})^2 \]
  \item<2-> Standard deviation of macro average can be computed as
    \[ \sigma_{\bar{y}} = \frac{\sigma}{\sqrt{k}} \approx \frac{s}{\sqrt{k}} \]
  \item<3-> Asymptotic 95\% confidence intervals are then given by
    \[ \bar{y} \pm 1.96 \cdot \sigma_{\bar{y}} \]
  \item<4-> Comparison of samples with Student's $t$-test, based on pooled cross-validation folds
    (feasible even for $n_1 = 1$)
  \end{itemize}
\end{frame}

\begin{frame}[c]
  \frametitle{Productivity measures with confidence intervals}
  \framesubtitle{\citep{Evert:Wankerl:Noeth:17}}

  \centering
  \hspace*{-8mm}%
  \begin{tabular}{@{}cc@{}}
    \only<beamer:1| handout:0>{\includegraphics[width=6cm]{img/murdoch_V_raw}}%
    \only<beamer:2-| handout:1>{\includegraphics[width=6cm]{img/murdoch_ttr_10k}}%
    & 
    \only<beamer:1| handout:0>{\includegraphics[width=6cm]{img/murdoch_H_raw}}%
    \only<beamer:2-| handout:1>{\includegraphics[width=6cm]{img/murdoch_H_10k}}%
    \\[5mm]
    \secondary{type count / TTR}%
    & \secondary{HonorÃ© $H$}%
    \\
    & \visible<3->{\primary{\small significance test vs.\ first 17 novels}}
    \\
    & \visible<3->{$t = -6.1$, df=5.52, $p = .0012$\primary{**}}
  \end{tabular}
\end{frame}


\begin{frame}[c]
  \frametitle{Cross-validated measures depend on fold size!}

  \centering\ungap[.4]
  \only<beamer:1| handout:1>{\includegraphics[width=10cm]{img/lexconst_binsize_b100}\hh{A}}%
  \only<beamer:2| handout:0>{\includegraphics[width=10cm]{img/lexconst_binsize_BNCS}\hh{B}}%
  \only<beamer:3| handout:2>{\includegraphics[width=10cm]{img/lexconst_binsize_BNC}\hh{C}}%
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame}
  \frametitle{Conclusion}

  \gap[1]
  
  \begin{center}
    \counterpoint{\Huge Thank you!}
  \end{center}

  \gap[2]
  
  \primary{\url{http://zipfR.R-Forge.R-Project.org/}}
  \begin{itemize}
  \item contains full \LaTeX{} source code of this presentation
  \item R package \texttt{zipfR} \citep{Evert:Baroni:07}\\ conveniently available from CRAN repository
  \end{itemize}

  \begin{flushright}
    \includegraphics[width=4cm]{img/zipfR_logo}
    %% \rule{1cm}{0mm}
  \end{flushright}
\end{frame}


\begin{frame}[c]
  \frametitle{My research programme for LNRE models}

  \begin{itemize}
  \item Improve efficiency \& numerical accuracy of implementation
    \begin{itemize}
    \item numerical integrals instead of differences of Gamma functions
    \item efficient generation of large random samples
    \end{itemize}
  \item Analyze accuracy of LNRE approximations
    \begin{itemize}
    \item comprehensive simulation experiments, esp.\ for small samples
    \end{itemize}
  \item Specify more flexible LNRE population models
    \begin{itemize}
    \item my favourite: piecewise Zipfian type density functions
    \item flexible approximation, but no deep mathematical justification
    \end{itemize}
  \item Develop hypothesis tests \& confidence intervals
    \begin{itemize}
    \item key challenge: goodness-of-fit \vs confidence region
    \item prediction intervals for model-based extrapolation
    \end{itemize}
  \item Simulation experiments for productivity measures
    \begin{itemize}
    \item Can we find a quantitative measure that is robust against confounding factors and corresponds to intuitive notions of productivity \& lexical diversity?
    \end{itemize}
  \end{itemize}
\end{frame}

\begin{frame}[c]
  \frametitle{My research programme for LNRE models}

  \begin{itemize}
  \item Is non-randomness a problem?
    \begin{itemize}
    \item not for morphological productivity \so \textsc{echo} correction
    \item tricky to include explicitly in LNRE approach
    \item[]
    \end{itemize}
  \item Do we need LNRE models for practical applications?
    \begin{itemize}
    \item better productivity measures + empirical sampling variation
    \item based on cross-validation approach \citep{Evert:Wankerl:Noeth:17}
    \item[]
    \end{itemize}
  \item How important is semantics \& context?
    \begin{itemize}
    \item Does it make sense to measure productivity and lexical diversity purely in terms of type-token distributions?
    \item e.g.\ register variation for morphological productivity
    \item type-token ratio $\neq$ complexity of author's vocabulary
    \end{itemize}
  \end{itemize}
\end{frame}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% References (if any)

\frame[allowframebreaks]{
  \frametitle{References}
  \bibliographystyle{natbib-stefan}
  \begin{scriptsize}
    \bibliography{stefan-publications,stefan-literature}
  \end{scriptsize}
}

\end{document}
